from __future__ import annotations
"""êµì •/í‡´ê³  ì—”ì§„ â€” ë§ì¶¤ë²•, ë¬¸ì²´ ì¼ê´€ì„±, ì¤‘ë³µ í‘œí˜„ ê²€ì‚¬ (ê·œì¹™ ê¸°ë°˜)"""

import re
import statistics
from collections import Counter
from dataclasses import dataclass, field
from typing import List, Dict, Tuple


# â”€â”€ ë§ì¶¤ë²• ê·œì¹™ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# (í‹€ë¦° í‘œí˜„, ì˜¬ë°”ë¥¸ í‘œí˜„, ì„¤ëª…)
SPELLING_RULES: List[Tuple[str, str, str]] = [
    (r"ë˜ìš”", "ë¼ìš”", "'ë˜ì–´ìš”'ì˜ ì¤„ì„ì€ 'ë¼ìš”'"),
    (r"ë¬", "ë", "'ë˜ì—ˆ'ì˜ ì¤„ì„ì€ 'ë'"),
    (r"í• ê»˜", "í• ê²Œ", "'í• ê²Œ'ê°€ ì˜¬ë°”ë¥¸ í‘œê¸°"),
    (r"í• êº¼", "í•  ê±°", "'í•  ê±°'ê°€ ì˜¬ë°”ë¥¸ í‘œê¸°"),
    (r"ëª‡ì¼", "ë©°ì¹ ", "'ë©°ì¹ 'ì´ ì˜¬ë°”ë¥¸ í‘œê¸°"),
    (r"ì–´ì˜ì—†", "ì–´ì´ì—†", "'ì–´ì´ì—†ë‹¤'ê°€ ì˜¬ë°”ë¥¸ í‘œê¸°"),
    (r"ê¸ˆìƒˆ", "ê¸ˆì„¸", "'ê¸ˆì„¸'ê°€ ì˜¬ë°”ë¥¸ í‘œê¸° (ê¸ˆì‹œì—ì˜ ì¤€ë§)"),
    (r"ì¼ì¼íˆ", "ì¼ì¼ì´", "'ì¼ì¼ì´'ê°€ ì˜¬ë°”ë¥¸ í‘œê¸°"),
    (r"ë°”ë¨", "ë°”ëŒ", "'ë°”ëŒ'ì´ ì˜¬ë°”ë¥¸ í‘œê¸° (ë°”ë¼ë‹¤ì˜ ëª…ì‚¬í˜•)"),
    (r"ì˜¤ë«ë§Œ", "ì˜¤ëœë§Œ", "'ì˜¤ëœë§Œ'ì´ ì˜¬ë°”ë¥¸ í‘œê¸°"),
    (r"ì˜¤ë«ë™ì•ˆ", "ì˜¤ë«ë™ì•ˆ", None),  # ì´ê±´ ì˜¬ë°”ë¦„ â€” skip
    (r"ì™ ì§€", "ì›¬ì§€", None),  # ë¬¸ë§¥ì— ë”°ë¼ ë‹¤ë¦„ â€” skip
    (r"ì–´ë–»ê²Œ ëœê±°", "ì–´ë–»ê²Œ ëœ ê±°", "ì˜ì¡´ëª…ì‚¬ 'ê±°'ëŠ” ë„ì–´ ì”€"),
    (r"ê°ˆê»€", "ê°ˆ ê±´", "'ê°ˆ ê±´'ì´ ì˜¬ë°”ë¥¸ í‘œê¸°"),
    (r"ì•Šë¼", "ì•ˆ ë¼", "'ì•ˆ ë¼'ê°€ ì˜¬ë°”ë¥¸ í‘œê¸°"),
    (r"ì•Šë˜", "ì•ˆ ë˜", "'ì•ˆ ë˜'ê°€ ì˜¬ë°”ë¥¸ í‘œê¸°"),
    (r"ëµˆìš”", "ë´¬ìš”", "'ëµˆì–´ìš”'ì˜ ì¤„ì„ì€ 'ë´¬ìš”'"),
    (r"ë°ë¡œ", "ëŒ€ë¡œ", "'ëŒ€ë¡œ'ê°€ ì˜¬ë°”ë¥¸ í‘œê¸° (ì˜ì¡´ëª…ì‚¬)"),
    (r"ë˜ì§€ê°„ì—", "ë“ ì§€ ê°„ì—", "'ë“ ì§€ ê°„ì—'ê°€ ì˜¬ë°”ë¥¸ í‘œê¸°"),
    (r"ìœ¼ë¯€ë¡œì¨", "ìœ¼ë¯€ë¡œ/ìœ¼ë¡œì¨", "'ìœ¼ë¯€ë¡œ'(ì´ìœ )ì™€ 'ìœ¼ë¡œì¨'(ìˆ˜ë‹¨) êµ¬ë¶„"),
    (r"í‹€ë¦¬ë‹¤([^.]*)(ë‹¤ë¥´|ì°¨ì´)", "ë‹¤ë¥´ë‹¤", "'í‹€ë¦¬ë‹¤'ì™€ 'ë‹¤ë¥´ë‹¤' í˜¼ìš© ì£¼ì˜"),
    (r"ë¬¸ì•ˆí•˜", "ë¬´ë‚œí•˜", "'ë¬´ë‚œí•˜ë‹¤'ê°€ ì˜¬ë°”ë¥¸ í‘œê¸°"),
    (r"ì„¤ë ˆì„", "ì„¤ë ˜", "'ì„¤ë ˜'ì´ ì˜¬ë°”ë¥¸ í‘œê¸°"),
    (r"ëŠ˜ê·¸ë§‰", "ëŠ˜ê·¸ë§‰", None),
    (r"í¬ì•ˆí•˜", "í¬í•œí•˜", "'í¬í•œí•˜ë‹¤'ê°€ ì˜¬ë°”ë¥¸ í‘œê¸°"),
]

# None ì„¤ëª…ì¸ í•­ëª© í•„í„°ë§
SPELLING_RULES = [(p, r, d) for p, r, d in SPELLING_RULES if d is not None]

# â”€â”€ í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²° ì–´ë¯¸ íŒ¨í„´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ENDING_PATTERNS = {
    "í•©ì‡¼ì²´": re.compile(r"(?:ìŠµë‹ˆë‹¤|ìŠµë‹ˆê¹Œ|ì‹­ì‹œì˜¤)[.?!]?\s*$"),
    "í•´ìš”ì²´": re.compile(r"(?:ì—ìš”|ì˜ˆìš”|ì–´ìš”|ì•„ìš”|ì£ |ë„¤ìš”|ëŠ”ìš”|ë‚˜ìš”|ë˜ìš”|ì„¸ìš”|ë˜ìš”|ë¼ìš”)[.?!]?\s*$"),
    "í•´ì²´(ë°˜ë§)": re.compile(r"(?:ì–´|ì•„|ì§€|ì•¼|ê±°ë“ |ì–ì•„|ëŠ”ë°|ë‹¤ê³ |ë˜|ëƒ)[.?!]?\s*$"),
    "í•˜ë¼ì²´": re.compile(r"(?:í•˜ë¼|ê±°ë¼|ì–´ë¼|ì•„ë¼)[.?!]?\s*$"),
    "í•˜ê²Œì²´": re.compile(r"(?:í•˜ê²Œ|í•˜ì„¸|í•˜ë„¤|ëŠ”ê°€|ë˜ê°€)[.?!]?\s*$"),
    "í•´ë¼ì²´": re.compile(r"(?:í•œë‹¤|ëŠ”ë‹¤|ì—ˆë‹¤|í–ˆë‹¤|ì˜€ë‹¤|ì¸ë‹¤|ë“ ë‹¤|ã„´ë‹¤|ê² ë‹¤|ë¦¬ë¼|ë”ë¼|êµ¬ë‚˜|ë¡œë‹¤)[.?!]?\s*$"),
    "ë‹¤ì²´(ì„œìˆ )": re.compile(r"(?:ì´ë‹¤|ì˜€ë‹¤|ì´ì—ˆë‹¤|ì´ë¼)[.?!]?\s*$"),
}


@dataclass
class SpellingIssue:
    line: int
    column: int
    wrong: str
    suggestion: str
    reason: str


@dataclass
class StyleStats:
    total_sentences: int = 0
    avg_length: float = 0.0
    std_length: float = 0.0
    min_length: int = 0
    max_length: int = 0
    ending_distribution: Dict[str, int] = field(default_factory=dict)
    dominant_ending: str = ""


@dataclass
class DuplicateGroup:
    ngram: str
    count: int
    positions: List[int]  # ë¬¸ì¥ ì¸ë±ìŠ¤


@dataclass
class ProofreadReport:
    """êµì • ë¶„ì„ ë¦¬í¬íŠ¸"""
    spelling_issues: List[SpellingIssue] = field(default_factory=list)
    style_stats: StyleStats = field(default_factory=StyleStats)
    duplicates: List[DuplicateGroup] = field(default_factory=list)
    summary: str = ""

    def to_dict(self) -> dict:
        return {
            "spelling_issues": [
                {"line": s.line, "column": s.column, "wrong": s.wrong,
                 "suggestion": s.suggestion, "reason": s.reason}
                for s in self.spelling_issues
            ],
            "style_stats": {
                "total_sentences": self.style_stats.total_sentences,
                "avg_length": round(self.style_stats.avg_length, 1),
                "std_length": round(self.style_stats.std_length, 1),
                "min_length": self.style_stats.min_length,
                "max_length": self.style_stats.max_length,
                "ending_distribution": self.style_stats.ending_distribution,
                "dominant_ending": self.style_stats.dominant_ending,
            },
            "duplicates": [
                {"ngram": d.ngram, "count": d.count, "positions": d.positions}
                for d in self.duplicates
            ],
            "summary": self.summary,
        }

    def to_markdown(self) -> str:
        parts: List[str] = ["# ğŸ“ êµì •/í‡´ê³  ë¦¬í¬íŠ¸\n"]

        # ë§ì¶¤ë²•
        parts.append(f"## ë§ì¶¤ë²• ({len(self.spelling_issues)}ê±´)")
        if self.spelling_issues:
            for s in self.spelling_issues[:30]:
                parts.append(f"- **L{s.line}**: `{s.wrong}` â†’ `{s.suggestion}` â€” {s.reason}")
            if len(self.spelling_issues) > 30:
                parts.append(f"  _(ì™¸ {len(self.spelling_issues) - 30}ê±´)_")
        else:
            parts.append("âœ… ë°œê²¬ëœ ë§ì¶¤ë²• ì˜¤ë¥˜ ì—†ìŒ")

        # ë¬¸ì²´
        ss = self.style_stats
        parts.append(f"\n## ë¬¸ì²´ í†µê³„")
        parts.append(f"- ì´ ë¬¸ì¥ ìˆ˜: {ss.total_sentences}")
        parts.append(f"- í‰ê·  ë¬¸ì¥ ê¸¸ì´: {ss.avg_length:.1f}ì (í‘œì¤€í¸ì°¨ {ss.std_length:.1f})")
        parts.append(f"- ìµœì†Œ/ìµœëŒ€: {ss.min_length} / {ss.max_length}ì")
        if ss.ending_distribution:
            parts.append("- ì¢…ê²° ì–´ë¯¸ ë¶„í¬:")
            for style, cnt in sorted(ss.ending_distribution.items(), key=lambda x: -x[1]):
                pct = cnt / max(ss.total_sentences, 1) * 100
                parts.append(f"  - {style}: {cnt}íšŒ ({pct:.0f}%)")

        # ì¤‘ë³µ
        parts.append(f"\n## ì¤‘ë³µ í‘œí˜„ ({len(self.duplicates)}ê±´)")
        if self.duplicates:
            for d in self.duplicates[:20]:
                parts.append(f"- **\"{d.ngram}\"** â€” {d.count}íšŒ ë°˜ë³µ")
        else:
            parts.append("âœ… ì£¼ìš” ì¤‘ë³µ í‘œí˜„ ì—†ìŒ")

        if self.summary:
            parts.append(f"\n## ìš”ì•½\n{self.summary}")

        return "\n".join(parts)


class Proofreader:
    """ê·œì¹™ ê¸°ë°˜ í•œêµ­ì–´ êµì •/í‡´ê³  ì—”ì§„"""

    # â”€â”€ ë§ì¶¤ë²• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @staticmethod
    def check_spelling(text: str) -> List[SpellingIssue]:
        issues: List[SpellingIssue] = []
        lines = text.split("\n")
        for line_no, line in enumerate(lines, 1):
            for pattern, suggestion, reason in SPELLING_RULES:
                for m in re.finditer(pattern, line):
                    issues.append(SpellingIssue(
                        line=line_no,
                        column=m.start() + 1,
                        wrong=m.group(),
                        suggestion=suggestion,
                        reason=reason,
                    ))
        return issues

    # â”€â”€ ë¬¸ì²´ ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @staticmethod
    def _split_sentences(text: str) -> List[str]:
        """ê°„ë‹¨í•œ í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬"""
        # ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œ + ê³µë°± ë˜ëŠ” ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬
        raw = re.split(r'(?<=[.?!])\s+', text)
        sentences = [s.strip() for s in raw if s.strip() and len(s.strip()) > 2]
        return sentences

    @staticmethod
    def analyze_style(text: str) -> StyleStats:
        sentences = Proofreader._split_sentences(text)
        if not sentences:
            return StyleStats()

        lengths = [len(s) for s in sentences]
        ending_counts: Dict[str, int] = {}
        for s in sentences:
            for style_name, pat in ENDING_PATTERNS.items():
                if pat.search(s):
                    ending_counts[style_name] = ending_counts.get(style_name, 0) + 1
                    break

        std = statistics.stdev(lengths) if len(lengths) > 1 else 0.0
        dominant = max(ending_counts, key=ending_counts.get) if ending_counts else "ë¶„ë¥˜ ë¶ˆê°€"

        return StyleStats(
            total_sentences=len(sentences),
            avg_length=statistics.mean(lengths),
            std_length=std,
            min_length=min(lengths),
            max_length=max(lengths),
            ending_distribution=ending_counts,
            dominant_ending=dominant,
        )

    # â”€â”€ ì¤‘ë³µ í‘œí˜„ íƒì§€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @staticmethod
    def find_duplicates(
        text: str,
        min_n: int = 3,
        max_n: int = 6,
        min_count: int = 3,
    ) -> List[DuplicateGroup]:
        """ì–´ì ˆ n-gram ê¸°ë°˜ ì¤‘ë³µ í‘œí˜„ íƒì§€"""
        sentences = Proofreader._split_sentences(text)
        ngram_positions: Dict[str, List[int]] = {}

        for idx, sent in enumerate(sentences):
            words = sent.split()
            for n in range(min_n, max_n + 1):
                for i in range(len(words) - n + 1):
                    gram = " ".join(words[i:i + n])
                    # ë„ˆë¬´ ì§§ì€ gram ì œì™¸
                    if len(gram) < 6:
                        continue
                    if gram not in ngram_positions:
                        ngram_positions[gram] = []
                    ngram_positions[gram].append(idx)

        # ë¹ˆë„ í•„í„°
        duplicates: List[DuplicateGroup] = []
        seen_substrings = set()
        for gram, positions in sorted(ngram_positions.items(), key=lambda x: -len(x[1])):
            unique_pos = sorted(set(positions))
            if len(unique_pos) < min_count:
                continue
            # ë” ê¸´ n-gramì— ì´ë¯¸ í¬í•¨ëœ ì§§ì€ ê²ƒì€ ì œì™¸
            if any(gram in longer for longer in seen_substrings):
                continue
            seen_substrings.add(gram)
            duplicates.append(DuplicateGroup(
                ngram=gram,
                count=len(unique_pos),
                positions=unique_pos,
            ))

        # ë¹ˆë„ìˆœ ì •ë ¬
        duplicates.sort(key=lambda d: -d.count)
        return duplicates[:50]

    # â”€â”€ í†µí•© ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @classmethod
    def analyze(cls, text: str) -> ProofreadReport:
        spelling = cls.check_spelling(text)
        style = cls.analyze_style(text)
        dupes = cls.find_duplicates(text)

        # ìš”ì•½ ìƒì„±
        summary_parts = []
        if spelling:
            summary_parts.append(f"ë§ì¶¤ë²• ì˜¤ë¥˜ {len(spelling)}ê±´ ë°œê²¬")
        if style.std_length > 30:
            summary_parts.append("ë¬¸ì¥ ê¸¸ì´ í¸ì°¨ê°€ í¼ (ì‚°ë§Œí•  ìˆ˜ ìˆìŒ)")
        if style.ending_distribution:
            dominant_pct = max(style.ending_distribution.values()) / max(style.total_sentences, 1) * 100
            if dominant_pct < 50:
                summary_parts.append("ì¢…ê²° ì–´ë¯¸ê°€ í˜¼ì¬ë¨ â€” ë¬¸ì²´ í†µì¼ ê²€í†  í•„ìš”")
        if dupes:
            summary_parts.append(f"ë°˜ë³µ í‘œí˜„ {len(dupes)}ê±´ â€” ë‹¤ë“¬ê¸° ê¶Œì¥")

        summary = "; ".join(summary_parts) if summary_parts else "ì „ë°˜ì ìœ¼ë¡œ ì–‘í˜¸í•©ë‹ˆë‹¤."

        return ProofreadReport(
            spelling_issues=spelling,
            style_stats=style,
            duplicates=dupes,
            summary=summary,
        )

    # â”€â”€ ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì˜ ë¬¸ì²´ ì¼ê´€ì„± ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @classmethod
    def compare_styles(cls, texts: Dict[str, str]) -> str:
        """ì—¬ëŸ¬ ì±•í„°/í…ìŠ¤íŠ¸ì˜ ë¬¸ì²´ë¥¼ ë¹„êµí•˜ì—¬ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ë°˜í™˜"""
        if not texts:
            return "ë¹„êµí•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."

        stats: Dict[str, StyleStats] = {}
        for name, txt in texts.items():
            stats[name] = cls.analyze_style(txt)

        parts = ["# ğŸ“Š ë¬¸ì²´ ì¼ê´€ì„± ë¹„êµ\n"]
        parts.append("| ì±•í„° | ë¬¸ì¥ ìˆ˜ | í‰ê·  ê¸¸ì´ | í‘œì¤€í¸ì°¨ | ì£¼ìš” ì–´ë¯¸ |")
        parts.append("|------|---------|----------|---------|----------|")
        for name, ss in stats.items():
            parts.append(
                f"| {name} | {ss.total_sentences} | {ss.avg_length:.1f} | "
                f"{ss.std_length:.1f} | {ss.dominant_ending} |"
            )

        # ì¼ê´€ì„± í‰ê°€
        avg_lengths = [ss.avg_length for ss in stats.values() if ss.total_sentences > 0]
        endings = [ss.dominant_ending for ss in stats.values() if ss.dominant_ending]

        if avg_lengths and len(avg_lengths) > 1:
            overall_std = statistics.stdev(avg_lengths)
            parts.append(f"\n**ì±•í„° ê°„ í‰ê·  ë¬¸ì¥ ê¸¸ì´ í¸ì°¨:** {overall_std:.1f}ì")
            if overall_std > 15:
                parts.append("âš ï¸ ì±•í„°ë§ˆë‹¤ ë¬¸ì¥ ê¸¸ì´ê°€ ìƒë‹¹íˆ ë‹¤ë¦…ë‹ˆë‹¤.")

        if endings:
            ending_counter = Counter(endings)
            most_common = ending_counter.most_common(1)[0]
            if most_common[1] < len(endings) * 0.6:
                parts.append("âš ï¸ ì¢…ê²° ì–´ë¯¸ ìŠ¤íƒ€ì¼ì´ ì±•í„°ë§ˆë‹¤ ë‹¬ë¼ ì¼ê´€ì„±ì´ ë‚®ìŠµë‹ˆë‹¤.")
            else:
                parts.append(f"âœ… ì „ë°˜ì ìœ¼ë¡œ **{most_common[0]}** ë¬¸ì²´ë¡œ ì¼ê´€ë©ë‹ˆë‹¤.")

        return "\n".join(parts)
